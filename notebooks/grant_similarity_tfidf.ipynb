{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNSF Grants Similarity: A Bag-of-Words Approach\n",
    "\n",
    "- Gabriel Okasa, Data Team, Swiss National Science Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "1) download publicly available text data from the SNSF data portal: [data.snf.ch](https://data.snf.ch/)\n",
    "\n",
    "2) pre-process the texts for the tf-idf model: english texts, lower casing, stop words and punctuation removal, stemming, n-grams\n",
    "\n",
    "3) apply the tf-idf weighting model and extract the tf-idf vectors\n",
    "\n",
    "4) compute the cosine similarity between the tf-idf vectors\n",
    "\n",
    "5) rank the texts based on the similarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "\n",
    "First, we import the neccessary libraries for data wrangling and natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import NLP/text libraries\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# import tfidf vectorizer and similarity metrics from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# and lanuage detection\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup stopwords dictionary\n",
    "\n",
    "In order to filter out stop words from the texts, we need to download the dictionary of stopwords available in the 'nltk' package (Bird et al., 2009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download / update stopwords dictionary\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import and Pre-Processing\n",
    "\n",
    "Data on SNSF grants is publicly available through the SNSF Data Portal: [data.snf.ch](https://data.snf.ch/), including text data, namely titles and abstracts of the grants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the text data from the SNSF data portal\n",
    "data = pd.read_csv('https://data.snf.ch/exportcsv/GrantWithAbstracts.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform some data wrangling first as we remove missing values and non-english texts, lower-case and concatenate the texts of titles and abstracts and reduce the dataset only to a subset of grants from year 2023 for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data, remove missing values, subset data\n",
    "data = data[['Title', 'Abstract', 'GrantNumber', 'CallDecisionYear']]\n",
    "# drop NAs as some grants do not have an abstract, e.g. not yet started grants\n",
    "data = data.dropna()\n",
    "\n",
    "# for demonstration purposes reduce only to year 2023\n",
    "data = data[data['CallDecisionYear'] == 2023]\n",
    "\n",
    "# concatenate titles and abstracts\n",
    "data['TitleAbstract'] = data.Title + '. ' + data.Abstract\n",
    "# lower case\n",
    "data['TitleAbstract'] = data.TitleAbstract.str.lower()\n",
    "# detect language of titles and abstracts\n",
    "data['Lang'] = data.TitleAbstract.apply(detect)\n",
    "# keep only english texts\n",
    "data = data[data.Lang == 'en']\n",
    "# extract texts as a list\n",
    "texts = data.TitleAbstract.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing and Tokenizer\n",
    "\n",
    "We begin the text pre-processing by removing the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation (string.punctuation: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "for text_idx in range(len(texts)):\n",
    "   texts[text_idx] = texts[text_idx].translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and further create the so-called unigrams by splitting the text sequence into separate words (tokens), while removing stop words and performing stemming of the remaining words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens as unigrams while removing stop words, use nltk english stopwords list\n",
    "# create empty list for storage\n",
    "tokens_unigrams = list()\n",
    "# and loop through all texts\n",
    "for text_idx in range(len(texts)):\n",
    "   tokens_unigrams.append([word for word in texts[text_idx].split() if word not in nltk.corpus.stopwords.words('english')])\n",
    "# perform stemming on unigrams\n",
    "# load the Porter stemmer: https://www.nltk.org/api/nltk.stem.porter.html\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "# and loop through all texts\n",
    "for token_idx in range(len(tokens_unigrams)):\n",
    "   # use again list comprehension to perform stemming word by word\n",
    "   tokens_unigrams[token_idx] = [ps.stem(word) for word in tokens_unigrams[token_idx]]\n",
    "   # keep only unigrams that have at least 2 characters, as otherwise the unigrams are not informative\n",
    "   tokens_unigrams[token_idx] = [word for word in tokens_unigrams[token_idx] if len(word) > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally to the unigrams, we create short word combinations called n-grams, up to n=3, i.e. combinations of 3 words. As with unigrams, we perform stemming, but keep the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text into n-grams as well, n is a tuning parameter (do not remove stop words here)\n",
    "n_grams = 3\n",
    "# only if n-grams are desired\n",
    "if n_grams > 1:\n",
    "   # create empty list for storage\n",
    "   tokens_ngrams = list()\n",
    "   # perform stemming first for all texts\n",
    "   for text_idx in range(len(texts)):\n",
    "      # stem words in text\n",
    "      tokens_ngrams.append([ps.stem(word) for word in texts[text_idx].split()])\n",
    "      # create n-grams from 2 up to n\n",
    "      tokens_ngrams[text_idx] = nltk.everygrams(tokens_ngrams[text_idx], 2, n_grams)\n",
    "      # and concatenate tuples and convert back to list of strings\n",
    "      tokens_ngrams[text_idx] = [' '.join(token_idx) for token_idx in list(tokens_ngrams[text_idx])]\n",
    "else:\n",
    "   # otherwise return just an empty list\n",
    "   tokens_ngrams = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we concatenate the unigrams with n-grams to complete the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate unigrams with n-grams to create the final vector of tokens\n",
    "tokens = [(list_unigrams + list_ngrams) for list_unigrams, list_ngrams in zip(tokens_unigrams, tokens_ngrams)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a numerical representation of the tokens, we apply the so-called TF-IDF (Term Frequency â€“ Inverse Document Frequency) weighting (Sparck Jones, 1972). TF-IDF is a type of bag-of-words approach, where the numerical representation of the text in vector space is based on a token decomposition of the text, ignoring the sequential nature of the text. This corresponds to the tokenization procedure conducted above. The TF-IDF then applies a weighting scheme that puts a higher weight on words that appear frequently in one document, but rarely across documents. The TF-IDF vectorization results in high-dimensional sparse vectors. Such TF-IDF vectorization has proven to be very effective in text similarity tasks, despite its simplicity (compare e.g. Shahmirzadi et al, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tfidf via scikit-learn \n",
    "# initiate the vectorizer (identity function for tokenizer and preprocessor as we already tokenized the texts)\n",
    "# specify l2 norm to get cosine similarity as dot product\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, use_idf=True, norm='l2')  \n",
    "# compute the tf-idf vector\n",
    "tfidf_vector = tfidf.fit_transform(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the most important words based on the TF-IDF weighting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first element of the tf-idf vector for the tokens with largest weight\n",
    "tfidf_df = pd.DataFrame(tfidf_vector[0].T.todense(), columns=[\"tf-idf\"], index = tfidf.get_feature_names_out())\n",
    "# sort the values\n",
    "tfidf_df = tfidf_df.sort_values('tf-idf', ascending=False)\n",
    "# check top 10\n",
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Metric\n",
    "\n",
    "In order to compare the similarity of the grants represented by the TF-IDF vectors, we compute the cosine distance between the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the similarity matrix among all grant texts\n",
    "text_similarity = pd.DataFrame(cosine_similarity(tfidf_vector),\n",
    "                               columns=data.GrantNumber).set_index(data.GrantNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the most similar grants relative to a target grant of interest, we rank-order the grants according to their cosine similarity. In what follows, let us review the text of an example grant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give grant number for which the best matches should be found\n",
    "grant_number = 221462\n",
    "# print the tile of this grant\n",
    "print(\"Text of the grant n. \" + str(grant_number) + \": \" +\n",
    "      str(data.TitleAbstract[data.GrantNumber == grant_number].to_list()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the similarity scores of all the other grants and return the top 5 most similar grants with their corresponding texts of titles and abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search top 5 most similar grants (1st one is the grant itself)\n",
    "top5_grants = text_similarity[grant_number].sort_values(ascending=False)[1:6].index.to_list()\n",
    "# and print the titles\n",
    "print(\"Top 5 most similar texts of grants are the following: \" + \"\\n\")\n",
    "# loop thorugh top5\n",
    "for grant_idx in range(len(top5_grants)):\n",
    "    print(\"Top \" + str(grant_idx+1) + \" most similar title is of the grant n. \" + str(top5_grants[grant_idx]) + \": \"\n",
    "          + str(data.TitleAbstract[data.GrantNumber == top5_grants[grant_idx]].to_list()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- data and text pre-processing is an integral part for bag-of-words approaches such as the TF-IDF vectors\n",
    "- TF-IDF weighting applies a higher weight to words and combination of words in the grant texts that appear often within the grant but rarely across the grants\n",
    "- the end-to-end pipeline for grant similarity retrieval can be implemented with just few lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \" O'Reilly Media, Inc.\".\n",
    "- Shahmirzadi, O., Lugowski, A., & Younge, K. (2019, December). Text similarity in vector space models: a comparative study. In 2019 18th IEEE international conference on machine learning and applications (ICMLA) (pp. 659-666). IEEE.\n",
    "- Sparck Jones, K. (1972). A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1), 11-21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
