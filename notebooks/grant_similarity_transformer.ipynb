{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNSF Grants Similarity: A Word Embeddings Approach\n",
    "\n",
    "- Gabriel Okasa, Data Team, Swiss National Science Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "1) download publicly available text data from the SNSF data portal: [data.snf.ch](https://data.snf.ch/)\n",
    "\n",
    "2) pre-process the texts for transformer model: english texts, lower casing, tokenization, string truncation\n",
    "\n",
    "3) apply pre-trained transformer model from [HuggingFace](https://huggingface.co/) and extract the embeddings via CLS token\n",
    "\n",
    "4) compute the cosine similarity between the text embeddings\n",
    "\n",
    "5) rank the texts based on the similarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "\n",
    "First, we import the neccessary libraries for data wrangling and natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import pytorch and transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# import similarity metrics from scikit-learn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# and lanuage detection\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU for faster computing\n",
    "\n",
    "Running deep learning models, such as the transformer models in this notebook, is more efficient using a GPU unit. Below we check for the availability of a GPU unit and set it as a primary device to perform the computations if available. Note, that for running PyTorch on a GPU, you must first install the CUDA toolkit: https://docs.nvidia.com/cuda/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "current_os = platform.system()\n",
    "# active device based on OS\n",
    "if current_os == 'Darwin':\n",
    "    # specify device as mps for Mac\n",
    "    device = 'mps'\n",
    "    print('MPS will be used as a device.')\n",
    "else:\n",
    "    # check if gpu is available, if yes use cuda, if not stick to cpu\n",
    "    # install CUDA here:https://pytorch.org/get-started/locally/\n",
    "    if torch.cuda.is_available():\n",
    "        # must be 'cuda:0', not just 'cuda', see: https://github.com/deepset-ai/haystack/issues/3160\n",
    "        device = torch.device('cuda:0')\n",
    "        print('GPU', torch.cuda.get_device_name(0) ,'is available and will be used as a device.')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('No GPU available, CPU will be used as a device instead.'\n",
    "              + 'Be aware that the computation time increases significantly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import and Pre-Processing\n",
    "\n",
    "Data on SNSF grants is publicly available through the SNSF Data Portal: [data.snf.ch](https://data.snf.ch/), including text data, namely titles and abstracts of the grants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the text data from the SNSF data portal\n",
    "data = pd.read_csv('https://data.snf.ch/exportcsv/GrantWithAbstracts.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform some data wrangling first as we remove missing values and non-english texts, lower-case and concatenate the texts of titles and abstracts and reduce the dataset only to a subset of grants from year 2023 for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data, remove missing values, subset data\n",
    "data = data[['Title', 'Abstract', 'GrantNumber', 'CallDecisionYear']]\n",
    "# drop NAs as some grants do not have an abstract, e.g. not yet started grants\n",
    "data = data.dropna()\n",
    "\n",
    "# for demonstration purposes reduce only to year 2023\n",
    "data = data[data['CallDecisionYear'] == 2023]\n",
    "\n",
    "# concatenate titles and abstracts\n",
    "data['TitleAbstract'] = data.Title + '. ' + data.Abstract\n",
    "# lower case\n",
    "data['TitleAbstract'] = data.TitleAbstract.str.lower()\n",
    "# detect language of titles and abstracts\n",
    "data['Lang'] = data.TitleAbstract.apply(detect)\n",
    "# keep only english texts\n",
    "data = data[data.Lang == 'en']\n",
    "# extract texts as a list\n",
    "texts = data.TitleAbstract.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a numerical representation of the texts, we rely on the word embeddings approach via transformers (Vaswani et al., 2017). In particular, we use the SPECTER model (Cohan et al., 2019), a BERT-type of model (Devlin et al., 2018), that has been pre-trained specifically on scientific texts and further augmented by citation graph.\n",
    "In such word embedding approach the numerical representation of the text in vector space is based on the full text sequence and takes the context into account thanks to the self-attention mechanism of transformers. The transformer vectorization results in high-dimension dense vectors. In order to access the pre-trained models such as SPECTER and use them for extracting the contextual embeddings, we rely on the HuggingFace platform: https://huggingface.co/ (Wolf et al., 2019). In what follows, we specify the model name and load the corresponding tokenizer and the model itself. Note, that for transformer models, no particular text pre-processing is neccessary, except lower-casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model name (SPECTER: BERT model pre-trained on scientific texts and augmented by a citation graph)\n",
    "model_name = 'allenai/specter2_base'\n",
    "# load the tokenizer and the model from HuggingFace and pass it to device\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the BERT tokenizer onto the grant texts. Due to the limited context window of the pre-trained models, the texts are truncated at the maximum length of 512 of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage for encoded texts\n",
    "encoded_text = list(range(len(texts)))\n",
    "# tokenize the texts\n",
    "for text_idx in range(len(encoded_text)):\n",
    "    # and send it to device\n",
    "    encoded_text[text_idx] = tokenizer(texts[text_idx],\n",
    "                                       max_length=512, # for BERT models the context window is limited to 512 tokens\n",
    "                                       truncation=True, # truncate the texts exceeding 512 tokens\n",
    "                                       padding='max_length', # pad shorter texts to 512 tokens\n",
    "                                       return_tensors = \"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "We then pass the tokenized texts through the pre-trained model and extract the so-called CLS token from the last hidden layer. The CLS token is a special token that provides an aggregate sequence representation (Devlin et al., 2018; Cohan et al., 2019) forming the text embedding. An alternative represenation could be the so-called mean pooling, i.e. averaging of all tokens from the last hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the CLS token (first special token summarizing the sentence level embeddings in BERT models)\n",
    "# storage for embeddings\n",
    "embeddings = {}\n",
    "# run the inputs through the model (sequentially to not overload CUDA memory in Jupyter)\n",
    "for text_idx in range(len(encoded_text)):\n",
    "    # first get the model output\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_text[text_idx])\n",
    "    # First element of model_output contains all token embeddings (last hidden state)\n",
    "    token_embeddings = output[0]\n",
    "    # extract the first out of 512 tokens, i.e. the so-called CLS token (torch dimension: [1,512,768])\n",
    "    cls_token = token_embeddings[:,0,:]\n",
    "    # normalize the CLS tokens with L2 norm to get similarity as dot product\n",
    "    cls_token = torch.nn.functional.normalize(cls_token, p=2, dim=1)\n",
    "    # pass back to CPU and convert to numpy array\n",
    "    embeddings[data.GrantNumber.iloc[text_idx]] = cls_token.detach().to('cpu').numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the embeddings and reformat them into a standard pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and save as pandas dataframe\n",
    "embeddings = pd.DataFrame(embeddings.items(), columns=['GrantNumber', 'TextEmbedding'])\n",
    "# and explode the list of embeddings into separate columns\n",
    "embeddings = pd.DataFrame(embeddings[\"TextEmbedding\"].tolist(),\n",
    "                          columns=list(range(token_embeddings.shape[2])),\n",
    "                          index=embeddings[\"GrantNumber\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Metric\n",
    "\n",
    "In order to compare the similarity of the grants represented by the SPECTER vectors, we compute the cosine distance between the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the similarity matrix among all grant texts\n",
    "text_similarity = pd.DataFrame(cosine_similarity(embeddings),\n",
    "                               columns=data.GrantNumber).set_index(data.GrantNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking\n",
    "\n",
    "To retrieve the most similar grants relative to a target grant of interest, we rank-order the grants according to their cosine similarity. In what follows, let us review the text of an example grant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give grant number for which the best matches should be found\n",
    "grant_number = 221462\n",
    "# print the tile of this grant\n",
    "print(\"Text of the grant n. \" + str(grant_number) + \": \"\n",
    "      + str(data.TitleAbstract[data.GrantNumber == grant_number].to_list()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the similarity scores of all the other grants and return the top 5 most similar grants with their corresponding texts of titles and abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search top 5 most similar grants (1st one is the grant itself)\n",
    "top5_grants = text_similarity[grant_number].sort_values(ascending=False)[1:6].index.to_list()\n",
    "# and print the titles\n",
    "print(\"Top 5 most similar texts of grants are the following: \" + \"\\n\")\n",
    "# loop thorugh top5\n",
    "for grant_idx in range(len(top5_grants)):\n",
    "    print(\"Top \" + str(grant_idx+1) + \" most similar title is of the grant n. \" + str(top5_grants[grant_idx]) + \": \"\n",
    "          + str(data.TitleAbstract[data.GrantNumber == top5_grants[grant_idx]].to_list()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- HuggingFace platform enables easy text pre-processing and embedding extraction for transformer approaches such as the BERT-type models\n",
    "- SPECTER model captures the context of the grant texts thanks to its specific pre-training on scientific texts and citation prediction task\n",
    "- the end-to-end pipeline for grant similarity retrieval can be implemented with just few lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Cohan, A., Feldman, S., Beltagy, I., Downey, D., & Weld, D. S. (2020). Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180.\n",
    "- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
    "- Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2019). Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
